{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/erictay1997/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/erictay1997/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(nltk.corpus.brown.tagged_sents(tagset=\"universal\")[:1000])\n",
    "# Change words to lower-case\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)):\n",
    "        sentence[i] = (sentence[i][0].lower(), sentence[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272\n",
      "2367\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter([x[0] for sentence in sentences for x in sentence])\n",
    "print(len(vocab))\n",
    "oov = set([x for x in vocab if vocab[x] == 1])\n",
    "print(len(oov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since a significant portion of vocab words only have count one, we set all words with count one to be out-of-vocabulary words. We do not set any more at risk of losing an even larger chunk of our training vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace word with 1 count with \"oov\"\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i][0] in oov:\n",
    "            sentence[i] = (\"oov\", sentence[i][1])\n",
    "vocab = set([x[0] for sentence in sentences for x in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos stores parts of speech as keys, and a counter of words as values\n",
    "pos = {}\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i][1] not in pos:\n",
    "            pos[sentence[i][1]] = Counter()\n",
    "        if sentence[i][0] not in pos[sentence[i][1]]:\n",
    "            pos[sentence[i][1]][sentence[i][0]] = 0\n",
    "        pos[sentence[i][1]][sentence[i][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET\n",
      "NOUN\n",
      "ADJ\n",
      "VERB\n",
      "ADP\n",
      ".\n",
      "ADV\n",
      "CONJ\n",
      "PRT\n",
      "PRON\n",
      "NUM\n",
      "X\n"
     ]
    }
   ],
   "source": [
    "for key in pos:\n",
    "    print(key)\n",
    "# We have 12 POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_index = {}\n",
    "start = 0\n",
    "for key in pos:\n",
    "    pos_to_index[key] = start\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'DET': 273,\n",
       "         '.': 103,\n",
       "         'PRON': 102,\n",
       "         'NOUN': 266,\n",
       "         'ADV': 34,\n",
       "         'ADP': 71,\n",
       "         'VERB': 45,\n",
       "         'NUM': 23,\n",
       "         'CONJ': 29,\n",
       "         'ADJ': 38,\n",
       "         'PRT': 16})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a counter of which POS tag starts each sentence\n",
    "initial = Counter()\n",
    "for sentence in sentences:\n",
    "    if sentence[0][1] not in initial:\n",
    "        initial[sentence[0][1]] = 0\n",
    "    initial[sentence[0][1]] += 1\n",
    "initial\n",
    "# Only 11 POS tags start our training sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial[\"X\"] = 0 # Add the last POS tag\n",
    "for pos_state in initial:\n",
    "    # Add 0.001 smoothing\n",
    "    initial[pos_state] += 0.001\n",
    "total_sum_initial = sum(initial.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_state is the initial state probabilities, \n",
    "# the probability distribution over 12 POS tags for the first word of a sentence\n",
    "initial_state = [0]*len(pos)\n",
    "for key in pos:\n",
    "    index = pos_to_index[key]\n",
    "    initial_state[index] = initial[key]/total_sum_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A is a state transition probability matrix\n",
    "# A[i][j] will represent the probability of transitioning from state i to state j\n",
    "# Add 0.001 smoothing\n",
    "A = [[0.001]*len(pos) for _ in range(len(pos))] \n",
    "# Store Counts\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)-1):\n",
    "        pos_0 = sentence[i][1]\n",
    "        pos_1 = sentence[i+1][1]\n",
    "        index_0 = pos_to_index[pos_0]\n",
    "        index_1 = pos_to_index[pos_1]\n",
    "        A[index_0][index_1] += 1\n",
    "# Normalize Probabilities\n",
    "for i in range(len(A)):\n",
    "    total_sum = sum(A[i])\n",
    "    A[i] = [x/total_sum for x in A[i]]\n",
    "A = np.matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "start = 0\n",
    "for word in vocab:\n",
    "    word_to_index[word] = start\n",
    "    start += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B is a observation probability matrix\n",
    "# B[i][j] will represent the probability of generating word j from state i\n",
    "# Add 0.001 smoothing\n",
    "B = [[0.001]*len(vocab) for _ in range(len(pos))] \n",
    "# Store Counts\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)):\n",
    "        word = sentence[i][0]\n",
    "        pos_for_word = sentence[i][1]\n",
    "        pos_index = pos_to_index[pos_for_word]\n",
    "        word_index = word_to_index[word]\n",
    "        B[pos_index][word_index] += 1\n",
    "# Normalize Probabilities\n",
    "for i in range(len(B)):\n",
    "    total_sum = sum(B[i])\n",
    "    B[i] = [x/total_sum for x in B[i]]\n",
    "B = np.matrix(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task one is complete\n",
    "# print(initial_state) # Initial State Distribution\n",
    "# print(A) # Transition Matrix\n",
    "# print(B) # Observation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params:\n",
    "# obs - observations [list of ints]\n",
    "# pi - the initial state probabilities [list of floats]\n",
    "# A - the state transition probability matrix [2D numpy array]\n",
    "# B - the observation probability matrix [2D numpy array]\n",
    "# returns states - the inferred state sequence [list of ints]\n",
    "\n",
    "def viterbi(obs, pi, A, B):\n",
    "    \n",
    "    # prob_dp[i][j] stores the maximal log probability of a state sequence of length j+1,\n",
    "    # ending in state i\n",
    "    # path_dp stores the path of prob_dp[i][j]\n",
    "    prob_dp = [[-float(\"Inf\")]*len(obs) for _ in range(A.shape[0])]\n",
    "    path_dp = [[None]*len(obs) for _ in range(A.shape[0])]\n",
    "    \n",
    "    for i in range(A.shape[0]):\n",
    "        prob_dp[i][0] = np.log(pi[i]) + np.log(B[i,obs[0]])\n",
    "        path_dp[i][0] = [i]\n",
    "        \n",
    "    for j in range(1, len(obs)):\n",
    "        for i in range(A.shape[0]):\n",
    "            for prev_i in range(A.shape[0]):\n",
    "                if (prob_dp[prev_i][j-1] + np.log(A[prev_i,i]) + np.log(B[i,obs[j]])) > prob_dp[i][j]:\n",
    "                    prob_dp[i][j] = prob_dp[prev_i][j-1] + np.log(A[prev_i,i]) + np.log(B[i,obs[j]])\n",
    "                    path_dp[i][j] = path_dp[prev_i][j-1]+[i]\n",
    "                    \n",
    "    maximum = max([x[len(obs)-1] for x in prob_dp])\n",
    "    for i in range(A.shape[0]):\n",
    "        if prob_dp[i][len(obs)-1] == maximum:\n",
    "            return path_dp[i][len(obs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(nltk.corpus.brown.tagged_sents(tagset=\"universal\")[10150:10153])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to lower-case, and unknown words to \"oov\"\n",
    "# Then convert words to ints, due to requirements of function\n",
    "for sentence in test:\n",
    "    for i in range(len(sentence)):\n",
    "        word = sentence[i][0].lower()\n",
    "        if word in vocab:\n",
    "            index = word_to_index[word]\n",
    "        else:\n",
    "            index = word_to_index[\"oov\"]\n",
    "        sentence[i] = (index, sentence[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = [x[0] for x in test[0]]\n",
    "labels_1 = [x[1] for x in test[0]]\n",
    "test_2 = [x[0] for x in test[1]]\n",
    "labels_2 = [x[1] for x in test[1]]\n",
    "test_3 = [x[0] for x in test[2]]\n",
    "labels_3 = [x[1] for x in test[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_pos = {}\n",
    "for pos in pos_to_index:\n",
    "    index_to_pos[pos_to_index[pos]] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words [list of ints]\n",
    "# true_labels [list of strings]\n",
    "\n",
    "def test(words, true_labels):\n",
    "    predicted_states = viterbi(words, initial_state, A, B)\n",
    "    predicted_labels = [index_to_pos[x] for x in predicted_states]\n",
    "    print(\"Predicted Labels: \\n{}\".format(predicted_labels))\n",
    "    print(\"True Labels: \\n{}\".format(true_labels))\n",
    "    print(\"Accuracy: {}{}\\n\\n\".format(np.mean([predicted_labels[i] == true_labels[i] for i in range(len(true_labels))])*100, \"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: \n",
      "['DET', 'VERB', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB', 'VERB', '.']\n",
      "True Labels: \n",
      "['DET', 'VERB', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB', 'VERB', '.']\n",
      "Accuracy: 100.0%\n",
      "\n",
      "\n",
      "Predicted Labels: \n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'ADP', 'NUM', 'DET', '.']\n",
      "True Labels: \n",
      "['DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'ADP', 'NUM', 'DET', '.']\n",
      "Accuracy: 94.44444444444444%\n",
      "\n",
      "\n",
      "Predicted Labels: \n",
      "['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'CONJ', 'DET', 'NOUN', '.']\n",
      "True Labels: \n",
      "['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'CONJ', 'DET', 'NOUN', '.']\n",
      "Accuracy: 100.0%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_1, labels_1)\n",
    "test(test_2, labels_2)\n",
    "test(test_3, labels_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm has close to 100% Accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
